# -*- coding: utf-8 -*-
"""Copy of Copy of Emotion-Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KwbBMW6KWOzRCKMJ22v6-JxWX6OVlpjh

https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?select=Twitter_Data.csv

# **1) Load Data**
"""

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from os import path

if path.exists('Twitter_Data.csv'):
  pass
else:
  uploaded = files.upload()

def run(file_name):
  df = pd.read_csv(file_name, delimiter=',')
  dtypes_df = pd.DataFrame(df.dtypes).reset_index().rename(columns={0:'dtype', 'index':'column_name'})
  print('Dataset Information:\n')
  df.info()
  print('='*30)
  print(f'Shape of our dataset: {df.shape}\n')
  print(f'There are {df.shape[1]} columns in the dataset')
  print(f'There are {df.shape[0]} rows in the dataset\n')
  cat_df = dtypes_df[dtypes_df['dtype']=='object']
  num_df = dtypes_df[dtypes_df['dtype']!='object']
  print(f"There are {len(cat_df)} Categorical columns > {list(cat_df['column_name'])}")
  print(f"There are {len(dtypes_df)-len(cat_df)} Numerical columns > {list(num_df['column_name'])}\n")
  print('Types of dataset:')
  display(dtypes_df.head(5))

  print(f'\nNumber of Duplicated: {df.duplicated().sum()}\n')
  print(f'Null values:\n{df.isnull().sum()}\n')
  print('Unique Values of Emotions')
  print(df.category.value_counts())
  print('\n')

  ax = (df['category']).value_counts().plot(kind='pie', title='Sentiments Pie Chart', figsize=(11, 6))
  ax.yaxis.set_visible(False)

  return df

data = run('Twitter_Data.csv')

"""# **2) Preprocessing**

The various text preprocessing steps are:

* Removing punctuations like . , ! $( ) * % @
* Numeric items to string
* Removing Stop words > Stop words are very commonly used words (a, an, the, etc.)
* Lower casing > Converting a word to lower case
* Tokenization > Splitting the sentence into words
* Stemming > a process of transforming a word to its root form
* Lemmatization > Lemmatization is preferred over Stemming (morphological analysis of the words)
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')

!pip install contractions

import string
import inflect
import re

from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
import contractions


porter_stemmer = PorterStemmer()
wordnet_lemmatizer = WordNetLemmatizer()

"""We can see that there are 13 different classes and some of the are having very few examples. (i.e. Anger, Boredom, Empty etc...). This is a very imbalanced dataset and it will not allow the model to converge. We'll reduce the number of classes."""

# Load Tweet dataset
df = pd.read_csv('/content/Twitter_Data.csv')
df = df[:60000]
df.head()

# Map tweet categories
df['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})

# Output first five rows
df.head()

"""### Cleaning the tweets"""

def tweet_to_words(tweet):
    ''' Convert tweet text into a sequence of words '''
    
    # convert to lowercase
    text = tweet.lower()
    # remove non letters
    text = re.sub(r"[^a-zA-Z0-9]", " ", text)
    # tokenize
    words = text.split()
    # remove stopwords
    words = [w for w in words if w not in stopwords.words("english")]
    # apply stemming
    #words = [PorterStemmer().stem(w) for w in words]
    words = [wordnet_lemmatizer.lemmatize(w) for w in words]
    # return list
    return words

print("\nOriginal tweet ->", df['clean_text'][1])
print("\nProcessed tweet ->", tweet_to_words(df['clean_text'][1]))

df = df.dropna()

df.shape

# Apply data processing to each tweet
X = list(map(tweet_to_words, df['clean_text']))

"""### Encoding the categories"""

from sklearn.preprocessing import LabelEncoder

# Encode target labels
le = LabelEncoder()
Y = le.fit_transform(df['category'])

print(X[0])
print(Y[0])

"""### Features and Target Column"""

from sklearn.model_selection import train_test_split
y = pd.get_dummies(df['category'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)

X_train[1]

from sklearn.feature_extraction.text import CountVectorizer
#from sklearn.feature_extraction.text import TfidfVectorizer

vocabulary_size = 5000

# Tweets have already been preprocessed hence dummy function will be passed in 
# to preprocessor & tokenizer step
count_vector = CountVectorizer(max_features=vocabulary_size,
#                               ngram_range=(1,2),    # unigram and bigram
                                preprocessor=lambda x: x,
                               tokenizer=lambda x: x) 
#tfidf_vector = TfidfVectorizer(lowercase=True, stop_words='english')

# Fit the training data
X_train = count_vector.fit_transform(X_train).toarray()

# Transform testing data
X_test = count_vector.transform(X_test).toarray()

X_train.shape

X_test[1]

# Plot the BoW feature vector
plt.plot(X_train[1,:])
plt.xlabel('twit')
plt.ylabel('Count')
plt.show()

# print first 200 words/tokens
print(count_vector.get_feature_names()[0:200])

from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
max_words = 5000
max_len=25

def tokenize_pad_sequences(text):
    '''
    This function tokenize the input text into sequnences of intergers and then
    pad each sequence to the same length
    '''
    # Text tokenization
    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')
    tokenizer.fit_on_texts(text)
    # Transforms text to a sequence of integers
    X = tokenizer.texts_to_sequences(text)
    # Pad sequences to the same length
    X = pad_sequences(X, padding='post', maxlen=max_len)
    # return sequences
    return X, tokenizer

    
X, tokenizer = tokenize_pad_sequences(df['clean_text'])

print('Before Tokenization & Padding \n', df['clean_text'][1])
print('After Tokenization & Padding \n', X[1])

X[1].shape

X_train[1].shape

import pickle

# saving
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# loading
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

#y = pd.get_dummies(df['category'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)
print('Train Set ->', X_train.shape, y_train.shape)
print('Validation Set ->', X_val.shape, y_val.shape)
print('Test Set ->', X_test.shape, y_test.shape)

import keras.backend as K

def f1_score(precision, recall):
    ''' Function to calculate f1 score '''
    
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout
from keras.metrics import Precision, Recall
from keras.optimizers import SGD
from keras.optimizers import RMSprop
from keras import datasets

from keras.callbacks import LearningRateScheduler
from keras.callbacks import History

from keras import losses

vocab_size = 5000
embedding_size = 32
epochs=20
learning_rate = 0.1
decay_rate = learning_rate / epochs
momentum = 0.8

sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)
# Build model
model= Sequential()
model.add(Embedding(vocab_size, embedding_size, input_length=max_len))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Bidirectional(LSTM(32)))
model.add(Dropout(0.4))
model.add(Dense(3, activation='softmax'))

import tensorflow as tf
tf.keras.utils.plot_model(model, show_shapes=True)

print(model.summary())

# Compile model
model.compile(loss='categorical_crossentropy', optimizer=sgd, 
               metrics=['accuracy', Precision(), Recall()])

# Train model

batch_size = 64
history = model.fit(X_train, y_train,
                      validation_data=(X_val, y_val),
                      batch_size=batch_size, epochs=epochs, verbose=1)

# Evaluate model on the test set
loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)
# Print metrics
print('')
print('Accuracy  : {:.4f}'.format(accuracy))
print('Precision : {:.4f}'.format(precision))
print('Recall    : {:.4f}'.format(recall))
print('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))

def plot_training_hist(history):
    '''Function to plot history for accuracy and loss'''
    
    fig, ax = plt.subplots(1, 2, figsize=(10,4))
    # first plot
    ax[0].plot(history.history['accuracy'])
    ax[0].plot(history.history['val_accuracy'])
    ax[0].set_title('Model Accuracy')
    ax[0].set_xlabel('epoch')
    ax[0].set_ylabel('accuracy')
    ax[0].legend(['train', 'validation'], loc='best')
    # second plot
    ax[1].plot(history.history['loss'])
    ax[1].plot(history.history['val_loss'])
    ax[1].set_title('Model Loss')
    ax[1].set_xlabel('epoch')
    ax[1].set_ylabel('loss')
    ax[1].legend(['train', 'validation'], loc='best')
    
plot_training_hist(history)

!pip install -U keras-tuner

from kerastuner.tuners import RandomSearch

from kerastuner import HyperModel


def build_model_2(hp):
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_size, input_length=max_len))
    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Bidirectional(LSTM(32)))
    model.add(Dropout(hp.Float('Dropout',min_value=0.1,max_value=0.4,step=0.1)))
    model.add(Dense(3, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=hp.Choice('optimizer',values=['adam', 'sgd' ],default='sgd'),metrics = ['accuracy', Precision(), Recall()])
    return model

tuner_model_2= RandomSearch(
        build_model_2,
        objective='val_accuracy',
        max_trials=5,directory='project',project_name='best model')

tuner_model_2.search_space_summary()



tuner_model_2.search(X_train, y_train, epochs=5, validation_data = (X_val, y_val))

tuner_model_2.get_best_hyperparameters()[0].values



best_model = tuner_model_2.get_best_models()[0]
best_model.evaluate(X_test, y_test)

history = best_model.fit(X_train, y_train,
                      validation_data=(X_val, y_val),
                      batch_size=64, epochs=5, verbose=1)

# Evaluate model on the test set
loss, accuracy, precision, recall = best_model.evaluate(X_test, y_test, verbose=0)
# Print metrics
print('')
print('Accuracy  : {:.4f}'.format(accuracy))
print('Precision : {:.4f}'.format(precision))
print('Recall    : {:.4f}'.format(recall))
print('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))

# # Save the model architecture & the weights
# model.save('best_model')
# print('Best model saved')

# with open('tokenizer.pickle', 'rb') as handle:
#     tokenizer = pickle.load(handle)
best_model.save('best_model.h5')
new_model = tf.keras.models.load_model('best_model.h5')

new_model.evaluate(X_test, y_test)

def predict(text):
    series = pd.Series(text)
    #tokenizer.fit_on_texts(series)
    X = pad_sequences(tokenizer.texts_to_sequences(series),
                      padding='post', maxlen=25)
    return int(np.argmax(new_model.predict(X))) - 1

predict('never underestimate the power stupid people')

new_test = list(map(predict, df['clean_text'][:100]))

new_test

real = list(df['category'][:100])

for i in zip(new_test, real):
  print(i)

